Domain decopmosition
======================

Overview
---------
 o init_domain_decomposition()
 o make_dd_communicators() separates PP/PME nodes
     cr->duty =  DUTY_PP (1) or DUTY_PME (2);
   then by using `cr->duty' as the group id, and calling MPI_Comm_split
     we get a communicator among all PP nodes.
     and another communicator among all PME nodes.

Files
------

### headers

Headers are under the directory `gromacs/legacyheaders` from GROMACS 5.0

types/commrec.h
gmx_ga2la.h

### source code

mdlib/domdec.c
mdlib/domdec_setup.c
mdlib/domdec_top.c
mdlib/domdec_con.c


Structures
----------

The object `dd` is the structure `gmx_domdec_t`,
defined in `types/commrec.h`.
Note it is contained in `cr->dd`.

```
struct gmx_domdec_t {
  MPI_Comm mpi_comm_all;
  int rank;
  /* global atom index to local index */
  gmx_ga2la_t gala;
}
```

```
typedef struct gmx_ga2la {
    gmx_bool      bAll; /* `laa` contains all atoms indices */
    int           mod;
    int           nalloc;
    gmx_laa_t    *laa; /* global index, `gmx_laa_t`, useful when `bAll` is true */
    gmx_lal_t    *lal; /* local index, `gmx_lal_t`, is the index */
} t_gmx_ga2la, *gmx_ga2la_t;
```




Functions
----------

### b = ga2la_get(ga2la, a_gl, &a_loc, &cell);

Compute the local index `a_loc` and cell id `cell` from the global index `a_gl`.
If `cell == 0`, atom is contains in this cell.
If `a_loc` is the local index.

This search goes through a hash table by mod,
so it should be quite fast in practice.



### b = ga2la_get_home(ga2la, a_gl, &a_loc);

Compute the local index `a_loc` from the global index `a_gl`.
If `a_loc` is the local index.
If cell != 0, return false.



### dd_collect_vec_sendrecv(dd, lv, v);

Collect values of the local vector `lv'
at different nodes, save to the global
vector `v' on the master.
Note, however, only the master node has
the complete vector in the end,
the local vector only does the sending.

  if ( !DDMASTER(dd) ) {
    /* send to the master */
    MPI_Send(lv, ..., DDMASTER(dd), ...);
  } else {
    /* copy data on this node */
    copy_rvec(lv, v);

    /* loop node by node */
    for (; n < dd->nnodes;)
      MPI_Recv(buf, ..., n, ...);
      /* copy from buf to v */
      copy_rvec(buf, v);
  }



### dd_collect_vec(dd, state_local, lv, v);

Collect local vector `lv` such that in the end
the master node has the complete vector `v`.

    dd_collect_cg(dd, state_local);
    dd_collect_vec_sendrecv(dd, lv, v);
  or
    dd_collect_vec_gatherv(dd, lv, v);


### dd_collect_state(dd, state_local, state);

  Copy data from `state_local` to `state`.
  Here `state` means the global state.

  On the master node, copy the following data to the global state
    * lambda[efptNR]
    * fep_state
    * veta
    * box
    * boxv
    * svir_prev

  Collect vectors
    * dd_collect_vec(dd, state_local->x, state->x);

    * dd_collect_vec(dd, state_local->v, state->v);

    * The variable `est` goes from 0 to estNR,
      EST_DISTR(e) is true for
        estX, estV, estSDX, estCGP, estLD_RNG, estLD_RNGI,
        estDISRE_INITF, estDISRE_RM3TAV,
        estORIRE_INITF, estORIRE_DTAV
      it is further filtered by state_local->flags.

    * For a usual simulation, state_local->flags is set for
        estBOX, estTC_INT, estX, estV



structures defined in domdec.c
-------------------------------
gmx_domdec_master_t;
gmx_domdec_ind_t;
gmx_domdec_com_dim_t;
gmx_domdec_root_t;
gmx_domdec_load_t;
gmx_cgsort_t;
gmx_domdec_sort_t;
vec_rvec_t;
gmx_ddpme_t;
gmx_ddzone_t;
gmx_domdec_comm_t;


### dd = init_domain_decomposition(cr, nc)

  copy_ivec(nc, dd->nc);
  dd_choose_grid(dd);



### dd_choose_grid(dd)

defined in domdec_setup.c

    if (cr->nnodes <= 10)
      cr->npmenodes = -1;
    else
      cr->npmenodes = guess_npme();

    if ( MASTER(cr) ) {
      optimize_ncells(dd->nc);
        * defined in domdec_setup.c,
          calls assign_factors(dd, ..., nc);
    }

    gmx_bcast(dd->nc);




### make_dd_communicators()

  copy_ivec(dd->nc, comm->ntot);
  if (cr->npmenodes > 0)
    split_communicator(cr);
  else
    cr->mpi_comm_mygroup = cr->mpi_comm_mysim;
  if (cr->duty & DUTY_PP)
    make_pp_communicator(cr);
  dd->pme_nodeid = dd_simnode2pmdnode(cr, cr->sim_nodeid);
  dd->ma = init_gmx_domdec_master_t();



### split_communicator()

  periods[0..DIM-1] = TRUE;
  MPI_Cart_create(cr->mpi_comm_mysim, DIM, comm->ntot, periods,
    reorder, &comm_cart);
  MPI_Comm_rank(comm_cart, &rank);
  cr->mpi_comm_mysim = comm_cart;
  cr->sim_nodeid = rank;

  cr->duty = DUTY_PP or DUTY_PME;
  MPI_Comm_split(cr->mpi_comm_mysim, cr->duty,
    dd_index(comm->ntot, dd->ci), &cr->mpi_mygroup);


### make_pp_communicator()

  MPI_Cart_create(cr->mpi_comm_mygroup, DIM, dd->nc, periods,
                  &comm_cart);
  cr->mpi_comm_mygroup = comm_cart;


### dd_partition_system()

Defined in gromacs/mdlib/domdec.c

The fourth parameter `bMasterState` means
if `state_global` is good on the master node.

Three switches (on line 9465)
```
if (bMasterState) {
  set_ddbox();
  get_cg_distribution();
  dd_distribute_state(); /* distribute coordinates here */
} else if (state_local->ddp_count < dd->ddp_count) {
  clear_dd_indices();
  rebuild_cgindex();
  make_dd_indices();
  set_ddbox();
} else {
  clear_dd_indices();
  set_ddbox();
}
``


### dd_distribute_state()

Defined in gromacs/mdlib/domdec.c

Copy information from state (state_global) to state_local
state_local->x
