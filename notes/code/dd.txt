Domain decopmosition
======================

Overview
---------
 o init_domain_decomposition()
 o make_dd_communicators() separates PP/PME nodes
     cr->duty =  DUTY_PP (1) or DUTY_PME (2);
   then by using `cr->duty' as the group id, and calling MPI_Comm_split
     we get a communicator among all PP nodes.
     and another communicator among all PME nodes.

Files
------

### headers

Headers are under the directory `gromacs/legacyheaders` from GROMACS 5.0

types/commrec.h
gmx_ga2la.h

### source code

mdlib/domdec.c
mdlib/domdec_setup.c
mdlib/domdec_top.c
mdlib/domdec_con.c


Structures
----------

The object `dd` is the structure `gmx_domdec_t`,
defined in `types/commrec.h`.
Note it is contained in `cr->dd`.

```
struct gmx_domdec_t {
    int                    nnodes;
    MPI_Comm               mpi_comm_all;
    int                    rank;
    int                    masterrank;

    ivec  nc;   /* number of cells along each direction */
    int   ndim; /* how many dimensions are involved in DD */
    ivec  dim;  /* dim[0..ndim-1]; so dim[d] gives the actual dimension */

    gmx_domdec_master     *ma; /* Only available on the master node */

    /* The local to gobal charge group index and local cg to local atom index */
    int   ncg_home;
    int   ncg_tot;
    int  *index_gl;
    int  *cgindex;

    int   nat_home; /* The number of home atoms */
    int   nat_tot; /* The total number of atoms: home and received zones */
    int  *gatindex; /* Index from the local atoms to the global atoms */
    int   gatindex_nalloc;

    /* Global atom number to local atom number list */
    gmx_ga2la_t ga2la;
};
```

### `dd->nc`

```
static void ddindex2xyz(ivec nc, int ind, ivec xyz)
{
  xyz[XX] = ind % nc[XX];
  xyz[YY] = (ind / nc[XX]) % nc[YY];
  xyz[ZZ] = ind / (nc[YY]*nc[XX]);
}
```

### `gmx_domdec_master`

```
typedef struct gmx_domdec_master
{
    /* The cell boundaries */
    real **cell_x;
    /* The global charge group division */
    int   *ncg;    /* Number of home charge groups for each node */
    int   *index;  /* Index of nnodes+1 into cg */
    int   *cg;     /* Global charge group index */
    int   *nat;    /* Number of home atoms for each node. */
    int   *ibuf;   /* Buffer for communication */
    rvec  *vbuf;   /* Buffer for state scattering and gathering */
} gmx_domdec_master_t;
```

```
struct gmx_domdec_t {
  MPI_Comm mpi_comm_all;
  int rank;
  /* global atom index to local index */
  gmx_ga2la_t gala;
}
```

```
typedef struct gmx_ga2la {
    gmx_bool      bAll; /* `laa` contains all atoms indices */
    int           mod;
    int           nalloc;
    gmx_laa_t    *laa; /* global index, `gmx_laa_t`, useful when `bAll` is true */
    gmx_lal_t    *lal; /* local index, `gmx_lal_t`, is the index */
} t_gmx_ga2la, *gmx_ga2la_t;
```




Functions
----------

### b = ga2la_get(ga2la, a_gl, &a_loc, &cell);

Compute the local index `a_loc` and cell id `cell` from the global index `a_gl`.
If `cell == 0`, atom is contains in this cell.
If `a_loc` is the local index.

This search goes through a hash table by mod,
so it should be quite fast in practice.



### b = ga2la_get_home(ga2la, a_gl, &a_loc);

Compute the local index `a_loc` from the global index `a_gl`.
If `a_loc` is the local index.
If cell != 0, return false.



### dd_collect_vec_sendrecv(dd, lv, v);

Collect values of the local vector `lv'
at different nodes, save to the global
vector `v' on the master.
Note, however, only the master node has
the complete vector in the end,
the local vector only does the sending.

  if ( !DDMASTER(dd) ) {
    /* slave sends `dd->nat_home` vectors to the master */
    MPI_Send(lv, dd->nat_home*sizeof(rvec), MPI_BTYE,
             DDMASTERRANK(dd), ...);
  } else {
    /* copy data on the master node: v[c] = lv[a++] */
    copy_rvec(lv[a++], v[c]);

    /* loop node by node */
    for (n = 0; n < dd->nnodes; n++) {
      MPI_Recv(buf, ma->nat[n]*sizeof(rvec), MPI_BYTE, n, ...);
      /* copy from buf to v: v[c] = buf[a++] */
      copy_rvec(buf[a++], v[c]);
    }
  }



### dd_collect_vec(dd, state_local, lv, v);

Collect local vector `lv` such that in the end
the master node has the complete vector `v`.

    dd_collect_cg(dd, state_local);

  if (dd->nnodes <= GMX_DD_NNODES_SENDRECV) { /* GMX_DD_NNODES_SENDRECV = 4 */
    dd_collect_vec_sendrecv(dd, lv, v);
  } else {
    dd_collect_vec_gatherv(dd, lv, v);
  }


### dd_collect_state(dd, state_local, state);

  Defined in `gromacs/mdlib/domdec.c`

  Copy data from `state_local` to `state` (global state on the master).

  On the master node, manually copy the following data
  from `state_local` to `state` (global state)
    * lambda[efptNR]
    * fep_state
    * veta, vol0
    * box, boxv
    * svir_prev, fvir_prev, pres_prev
    * nosehoover_xi[], nosehoover_vxi[]
    * nhpres_xi[], nhpres_vxi[]

  Collect vectors from different nodes
    * dd_collect_vec(dd, state_local, state_local->x, state->x);
    * dd_collect_vec(dd, state_local, state_local->v, state->v);
    * dd_collect_vec(dd, state_local, state_local->sd_X, state->sd_X);
    * dd_collect_vec(dd, state_local, state_local->cg_p, state->cg_p);

    * The number of atoms is assumed to be saved in
      `dd->nat_home` and `dd->ma->nat[n]`

    * The copying of the above quantities are done in a loop over `est`
      The variable `est` goes from 0 to estNR,
      EST_DISTR(e) is true for
        estX, estV, estSDX, estCGP, estLD_RNG, estLD_RNGI,
        estDISRE_INITF, estDISRE_RM3TAV,
        estORIRE_INITF, estORIRE_DTAV
      it is further filtered by state_local->flags.

    * For a usual simulation, state_local->flags is set for
        estBOX, estTC_INT, estX, estV



structures defined in domdec.c
-------------------------------
gmx_domdec_master_t;
gmx_domdec_ind_t;
gmx_domdec_com_dim_t;
gmx_domdec_root_t;
gmx_domdec_load_t;
gmx_cgsort_t;
gmx_domdec_sort_t;
vec_rvec_t;
gmx_ddpme_t;
gmx_ddzone_t;
gmx_domdec_comm_t;


### dd = init_domain_decomposition(cr, nc)

  copy_ivec(nc, dd->nc);
  dd_choose_grid(dd);



### dd_choose_grid(dd)

defined in domdec_setup.c

    if (cr->nnodes <= 10)
      cr->npmenodes = -1;
    else
      cr->npmenodes = guess_npme();

    if ( MASTER(cr) ) {
      optimize_ncells(dd->nc);
        * defined in domdec_setup.c,
          calls assign_factors(dd, ..., nc);
    }

    gmx_bcast(dd->nc);




### make_dd_communicators()

  copy_ivec(dd->nc, comm->ntot);
  if (cr->npmenodes > 0)
    split_communicator(cr);
  else
    cr->mpi_comm_mygroup = cr->mpi_comm_mysim;
  if (cr->duty & DUTY_PP)
    make_pp_communicator(cr);
  dd->pme_nodeid = dd_simnode2pmdnode(cr, cr->sim_nodeid);
  dd->ma = init_gmx_domdec_master_t();



### split_communicator()

  periods[0..DIM-1] = TRUE;
  MPI_Cart_create(cr->mpi_comm_mysim, DIM, comm->ntot, periods,
    reorder, &comm_cart);
  MPI_Comm_rank(comm_cart, &rank);
  cr->mpi_comm_mysim = comm_cart;
  cr->sim_nodeid = rank;

  cr->duty = DUTY_PP or DUTY_PME;
  MPI_Comm_split(cr->mpi_comm_mysim, cr->duty,
    dd_index(comm->ntot, dd->ci), &cr->mpi_mygroup);


### make_pp_communicator()

  MPI_Cart_create(cr->mpi_comm_mygroup, DIM, dd->nc, periods,
                  &comm_cart);
  cr->mpi_comm_mygroup = comm_cart;


### dd_partition_system()

Defined in gromacs/mdlib/domdec.c

The fourth parameter `bMasterState` is explained below

  * If `bMasterState == TRUE`, `state_global` is good on the master node,
    but `state_local` can be messy.

  * If `bMasterState == FALSE`, `state_local` is fine (roughly).

This function is used in the MD main loop in `md.c`

  * bMasterState is TRUE in the call near the end of the MD step.

  * In the neighbor-search step, bMasterState is usually FALSE,
    unless the box is dynamic, and it becomes too skrewed and
    a correction to box is made.
    Note it is then `dd_collect_state()` will be called.


Three switches (on line 9465)
```
if (bMasterState) {
  /* enters this branch near the end of MD step */

  clear_dd_indices(dd, 0, 0);
  set_ddbox();
  get_cg_distribution();

  /* distribute coordinates here
     from `state` (global state) to `state_local` */
  dd_distribute_state();

} else if (state_local->ddp_count < dd->ddp_count) {
  clear_dd_indices(dd, 0, 0);
  rebuild_cgindex();
  make_dd_indices();
  set_ddbox();

} else {
  /* enters this branch in the neighbor-search branch */

  clear_dd_indices(dd, dd->ncg_home, dd->nat_home);
  set_ddbox();

  bBoxChanged = TRUE;
  bRedist = TRUE; /* request sorting of charge groups */
}
```

Then we have

```
set_dd_cell_sizes();

if ( bRedist ) {
  /* distribute charge groups, calls `calc_cg_move(state_local)` */
  dd_redistribute_cg(stat_local);
}

/* compute `cell_ns_x0[0..2]` and `cell_ns_x1[0..2]` */
get_nsgrid_boundaries(cell_ns_x0, cell_ns_x1);

if ( bBoxChanged ) {
  comm_dd_ns_cell_sizes();
}

if ( bSortCG ) {
  ...
  dd_sort_state(state_local);
}

setup_dd_communication();

make_dd_indices();

set_cg_boundaries();

dd_make_local_top(top_local);

state_local->natoms = comm->nat[ddnatNR-1];
dd_realloc_state(state_local, f, state_local->natoms);

atoms2md(dd->gaindex, dd->nat_home, mdatoms);

if ( (cr->duty & DUTY_PME) == 0 ) {
  gmx_pme_send_parameters();
}

```



### clear_dd_indices(dd, ncg0, nat0)

delete local indices from `ncg0` for charge groups and `nat0` for atoms


### dd_redistribute_cg()

* calc_cg_move(state, move)


### calc_cg_move(state, move)

defined in `mdlib/domdec.c`

* for each charge group compute the center of mass `cm_new`.

* set dev[0..2] to represent if it crosses the cell boundary;
  dev[d] = 1 for overflow, or -1 for underflow
  (line 4519, line 4546).

* wrap the coordinates into the box (lines 4566-4584).

* cg_cm[cg] = cm_new

* compute `mc` and `flag`
  o `flag` is a combination of DD_FLAG_FW(d) or DD_FLAG_BW(d) for each dimension d
  o `mc` is equal to `d*2` or `d*2 + 1` for
    the lowest dimension that requires a move
  o if the charge group `cg` is fine, `flag = 0, mc = -1;`

* move[cg] = mc + flag;
  o if `move[cg] < 0`, don't move it


### dd_sort_state(state)

* order_vec_atom();
* dd->nat_home = dd->cgindex[dd->ncg_home];



### dd_distribute_state()

Defined in gromacs/mdlib/domdec.c

Copy information from state (state_global) to state_local
state_local->x
